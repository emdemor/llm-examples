{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90793a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install GitPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8521bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Insert your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0000c9",
   "metadata": {},
   "source": [
    "# Repositório Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ca2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo, GitCommandError\n",
    "\n",
    "\n",
    "try:\n",
    "    repo = Repo.clone_from(\n",
    "        \"https://github.com/hwchase17/langchain\", to_path=\"./data/test_repo1\"\n",
    "    )\n",
    "except GitCommandError:\n",
    "    pass\n",
    "\n",
    "repo = Repo(\"./data/test_repo1\")\n",
    "    \n",
    "branch = repo.head.reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cf1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader\n",
    "loader = GitLoader(repo_path=\"./data/test_repo1/\", branch=branch, )\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5b4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=50, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704e2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"clips/mfaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1393cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# -- demora ----------------------------\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "vector_store.save_local(\"data/git-huggingface.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75812e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f214b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 544 ms, total: 2.23 s\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"clips/mfaq\")\n",
    "vector_store = FAISS.load_local(\"data/git-huggingface.db\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749410bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains  import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Escolha o modelo de lingaugem\n",
    "llm = OpenAI(model='text-davinci-003', temperature=0)\n",
    "\n",
    "# Instancie o retriever\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19710c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O prompt para perguntas e respostas é definido usando o PromptTemplate, que é responsável por construir um valor de prompt. Estes PromptTemplates podem fazer coisas como formatação, seleção de exemplos e mais. Em um nível alto, esses são basicamente objetos que expõem um método `format_prompt` para construir um prompt. No nível interno, QUALQUER coisa pode acontecer.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Como é definido o prompt para perguntas e respostas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedb840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Você precisa alterar o arquivo \"conversational_retrieval_chain.py\" para customizar o prompt de ConversationalRetrievalChain.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Qual é o arquivo que tenho que alterar para customizar o prompt de ConversationalRetrievalChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7453cd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O arquivo conversational_retrieval_chain.py está localizado na pasta langchain/chains/conversational_retrieval_chain.py.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Qual é o caminho do arquivo conversational_retrieval_chain.py?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec80a4",
   "metadata": {},
   "source": [
    "# ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3626923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c071ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vector_store.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251767f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Você precisa alterar o arquivo de configuração do ConversationalRetrievalChain para customizar o prompt.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Qual é o arquivo que tenho que alterar para customizar o prompt de ConversationalRetrievalChain?\"\n",
    "result = qa({\"question\": query})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e153b9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Não sei.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"qual é o caminho desse arquivo?\"\n",
    "result = qa({\"question\": query})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "462807da",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50c15f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O arquivo que você precisa alterar para customizar o prompt da classe ConversationalRetrievalChain é o langchain/prompts.py.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Qual é o arquivo que tenho que alterar para customizar o prompt da classe ConversationalRetrievalChain?\"\n",
    "result = qa({\"question\": query})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca7f0902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Esse arquivo existe?\"\n",
    "result = qa({\"question\": query})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70b6688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Você pode usar o arquivo langchain/prompts/prompt.py para customizar o prompt da classe ConversationalRetrievalChain.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Eu verifiquei e ele não existe. Pode me dar uma resposta alternativa?\"\n",
    "result = qa({\"question\": query})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42152f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83156a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de651074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
