{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a708cb78-3ee8-4cea-922e-372ec702c87b",
   "metadata": {},
   "source": [
    "# Video Summary\n",
    "\n",
    "#### Adapted from: https://github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/video_summary.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08377fe3-d419-447d-a8f7-25879f400916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:05:15.425287Z",
     "iopub.status.busy": "2024-08-04T17:05:15.424819Z",
     "iopub.status.idle": "2024-08-04T17:05:15.427282Z",
     "shell.execute_reply": "2024-08-04T17:05:15.426883Z",
     "shell.execute_reply.started": "2024-08-04T17:05:15.425268Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain youtube-transcript-api tiktoken pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc6e3c8-c468-496b-8f0c-e44756417cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:27:09.189114Z",
     "iopub.status.busy": "2024-08-04T22:27:09.188882Z",
     "iopub.status.idle": "2024-08-04T22:27:09.193853Z",
     "shell.execute_reply": "2024-08-04T22:27:09.193397Z",
     "shell.execute_reply.started": "2024-08-04T22:27:09.189093Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdeefbe5-66cd-4ab6-860f-bed3abcc537d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:26:48.390811Z",
     "iopub.status.busy": "2024-08-04T22:26:48.390580Z",
     "iopub.status.idle": "2024-08-04T22:26:48.626219Z",
     "shell.execute_reply": "2024-08-04T22:26:48.625778Z",
     "shell.execute_reply.started": "2024-08-04T22:26:48.390789Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=5t1vTLU7s40\", add_video_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1247fa17-1883-4a7c-b085-5b4bee3a9e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:26:50.140284Z",
     "iopub.status.busy": "2024-08-04T22:26:50.140089Z",
     "iopub.status.idle": "2024-08-04T22:26:52.428195Z",
     "shell.execute_reply": "2024-08-04T22:26:52.427769Z",
     "shell.execute_reply.started": "2024-08-04T22:26:50.140271Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the youtube video caption into Documents\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da474e24-f779-4f08-b536-771e3c0c644e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:26:53.560527Z",
     "iopub.status.busy": "2024-08-04T22:26:53.560378Z",
     "iopub.status.idle": "2024-08-04T22:26:53.564811Z",
     "shell.execute_reply": "2024-08-04T22:26:53.564513Z",
     "shell.execute_reply.started": "2024-08-04T22:26:53.560514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142689,\n",
       " \"- I see the danger of this\\nconcentration of power through proprietary AI systems as a much bigger danger\\nthan everything else. What works against this is people who think that\\nfor reasons of security, we should keep AI systems\\nunder lock and key because it's too dangerous to put it in the hands of e\",\n",
       " 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many characters in the doc and some content\n",
    "len(docs[0].page_content), docs[0].page_content[:300], len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339200f-1e23-4ffc-8556-d399e9ad068d",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f09b37e-08e8-4d7d-82b7-7dead7cd4463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:26:55.957685Z",
     "iopub.status.busy": "2024-08-04T22:26:55.957532Z",
     "iopub.status.idle": "2024-08-04T22:26:56.264987Z",
     "shell.execute_reply": "2024-08-04T22:26:56.264521Z",
     "shell.execute_reply.started": "2024-08-04T22:26:55.957671Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96efbfa-5c3c-41d1-bd85-8ade4211bbe4",
   "metadata": {},
   "source": [
    "## Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab08eda-59fa-4086-ac9e-3d2808a4067d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:27:12.881471Z",
     "iopub.status.busy": "2024-08-04T22:27:12.881092Z",
     "iopub.status.idle": "2024-08-04T22:27:15.865402Z",
     "shell.execute_reply": "2024-08-04T22:27:15.865058Z",
     "shell.execute_reply.started": "2024-08-04T22:27:12.881454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 1118\n",
      "\tPrompt Tokens: 890\n",
      "\tCompletion Tokens: 228\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0002703\n",
      "content='O texto discute a concentração de poder em sistemas de inteligência artificial (IA) proprietários, que é vista como um grande perigo. O autor argumenta que manter esses sistemas sob controle restrito, por motivos de segurança, pode resultar em um futuro negativo, onde poucas empresas controlam toda a informação. Ele acredita que as pessoas são fundamentalmente boas e que a IA, especialmente a de código aberto, pode potencializar essa bondade. Yann LeCun, cientista-chefe de IA da Meta e um dos principais nomes na área, defende a abertura no desenvolvimento de IA e critica aqueles que alertam sobre os perigos da inteligência geral artificial (AGI). LeCun acredita que a AGI será criada um dia, mas que será benéfica e não escapará ao controle humano. Ele também discute as limitações dos modelos de linguagem autoregressivos, como o GPT-4 e LLaMA, afirmando que, embora sejam úteis, não são o caminho para alcançar uma inteligência super-humana, pois carecem de características essenciais como compreensão do mundo físico, memória persistente, raciocínio e planejamento.' response_metadata={'token_usage': {'completion_tokens': 228, 'prompt_tokens': 890, 'total_tokens': 1118}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-33b40e14-dc93-496a-a469-2b4b3eb941fe-0' usage_metadata={'input_tokens': 890, 'output_tokens': 228, 'total_tokens': 1118}\n"
     ]
    }
   ],
   "source": [
    "text = docs[0].page_content[:4000]\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    summary = llm.invoke(f\"Por favor, façar um resumo em portugues do texto abaixo: {text}.\")\n",
    "    print(cb)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6a70b-3db7-4b7c-be90-2b563b00593d",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7330ca8-bf7f-41cf-8e84-3e63b360b127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:27:26.856151Z",
     "iopub.status.busy": "2024-08-04T22:27:26.855917Z",
     "iopub.status.idle": "2024-08-04T22:27:27.102861Z",
     "shell.execute_reply": "2024-08-04T22:27:27.102383Z",
     "shell.execute_reply.started": "2024-08-04T22:27:26.856130Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# we need to split the long input text\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26475eea-fd36-4cd4-bd2f-1131a0dfc2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T00:50:56.468192Z",
     "iopub.status.busy": "2024-08-05T00:50:56.467806Z",
     "iopub.status.idle": "2024-08-05T00:50:56.470378Z",
     "shell.execute_reply": "2024-08-05T00:50:56.469959Z",
     "shell.execute_reply.started": "2024-08-05T00:50:56.468168Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    summary = chain.invoke(split_docs)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc3758d-05db-4e7e-ae18-62565f51ba9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T22:58:47.470907Z",
     "iopub.status.busy": "2024-08-04T22:58:47.470692Z",
     "iopub.status.idle": "2024-08-04T22:58:47.473934Z",
     "shell.execute_reply": "2024-08-04T22:58:47.473545Z",
     "shell.execute_reply.started": "2024-08-04T22:58:47.470893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The original summary is comprehensive and captures the key points made by Yann LeCun regarding the future of AI, the limitations of current models, and the importance of open-source approaches. The additional context provided does not significantly alter or enhance the existing summary, as it reiterates themes of human goodness, the transformative potential of AI, and the need for open-source solutions. Therefore, the original summary remains relevant and effective in conveying LeCun's insights.\n",
       "\n",
       "**Final Summary:**\n",
       "\n",
       "Yann LeCun, chief AI scientist at Meta, emphasizes the dangers of concentrating power in proprietary AI systems, advocating for open-source AI to empower individuals. He believes in the fundamental goodness of people and argues that open-source AI can enhance this goodness. LeCun critiques the reliance on autoregressive large language models (LLMs) like GPT-4 for achieving superhuman intelligence, stating they lack essential characteristics of intelligent behavior, such as understanding the physical world, persistent memory, reasoning, and planning. He argues that most of our knowledge is derived from direct interaction with the physical world rather than language, highlighting that sensory input provides a richer and more immediate learning experience. While acknowledging the usefulness of LLMs, he contends they are insufficient for developing human-level intelligence compared to the vast experiential learning of a child.\n",
       "\n",
       "LeCun also notes that intelligence must be grounded in reality, whether physical or simulated, and that many tasks we take for granted, such as driving or performing household chores, require a level of embodied understanding that LLMs currently lack. He explains that the training process of LLMs, which involves predicting the next word in a sequence based on prior words, limits their ability to engage in deeper reasoning or intuitive understanding of the physical world. This autoregressive approach contrasts with human cognition, where abstract thinking occurs independently of language, allowing for a more nuanced understanding of concepts before they are expressed verbally.\n",
       "\n",
       "Furthermore, LeCun argues that true understanding of the world requires a sophisticated internal world model, which LLMs do not possess. He asserts that while it is possible to build a world model through prediction, relying solely on language is inadequate due to its low bandwidth. Instead, he emphasizes the importance of observing the world and understanding its dynamics to create a model that can predict future states based on potential actions. He highlights the challenges in developing generative models for video, noting that predicting the distribution of frames in a video is significantly more complex than predicting the next word in a sequence. LeCun points out that the world is far more complicated and rich in information than text, as video represents high-dimensional continuous spaces, making it difficult for models to predict intricate details, such as textures and features in a scene.\n",
       "\n",
       "He critiques existing methods, such as training systems to reconstruct images from corrupted versions, as largely ineffective for developing robust representations of visual information. Instead, he advocates for alternative approaches, like joint embedding predictive architecture (JEPA), which involves taking both the full and corrupted versions of an image, running them through encoders, and training a predictor to derive the representation of the full input from the corrupted one. LeCun explains that JEPA focuses on extracting abstract representations rather than predicting all pixel details, making it a more efficient method for capturing essential information. He acknowledges that while JEPA is a step towards advanced machine intelligence, it is not a complete solution, as it still requires further development to achieve the level of understanding and reasoning found in human cognition.\n",
       "\n",
       "LeCun further elaborates that effective learning requires abstraction, where unnecessary details are filtered out to focus on essential features of the environment. He argues that while language provides a level of abstraction, it can also lead to a reliance on simplified representations that may not capture the complexity of the physical world. He warns against prematurely combining visual and language models, as this could lead to superficial improvements without achieving the deeper understanding exhibited by non-linguistic animals. Ultimately, he believes that developing a robust understanding of the world through sensory input is crucial before integrating language into AI systems.\n",
       "\n",
       "In his exploration of advanced learning techniques, LeCun discusses non-contrastive methods such as BYOL, VICReg, and I-JEPA, which focus on training systems to predict representations of uncorrupted inputs from corrupted ones. He describes how these methods involve corrupting images through transformations like cropping or blurring and then training a predictor to derive the original representation. He also introduces V-JEPA, an extension of I-JEPA applied to video, which masks segments of frames across multiple time steps to learn effective representations of video data. These approaches aim to enhance the system's ability to learn common sense and improve its understanding of the world, ultimately contributing to the development of more sophisticated AI systems.\n",
       "\n",
       "LeCun further explains that these advanced models can help determine the physical plausibility of video sequences, allowing the system to recognize when objects appear, disappear, or change shape in ways that defy physical laws. He suggests that with modifications, such as predicting future states based on actions taken (like steering a car), these models could eventually support complex tasks like driving. He emphasizes the importance of creating an internal model that can predict outcomes based on actions, which is essential for planning and decision-making. This model predictive control approach allows for the planning of sequences of actions to achieve specific objectives, a capability that LLMs currently lack. LeCun acknowledges that while hierarchical planning is necessary for complex tasks, it requires specific architectural designs to emerge from these foundational models.\n",
       "\n",
       "In discussing the limitations of LLMs, LeCun illustrates that while they can generate plans and answer questions at a certain level of abstraction, they struggle with detailed, physical actions, such as the precise steps needed to stand up from a chair or navigate complex environments. He emphasizes that LLMs depend on their training data and may produce hallucinated or non-factual responses when faced with unfamiliar scenarios. He explains that the autoregressive nature of LLMs leads to an exponential increase in the likelihood of nonsensical outputs as the model generates more tokens, due to the accumulation of errors. This highlights a fundamental flaw in LLMs, as they can only perform well on prompts they have been trained on, while the vast majority of possible prompts remain unaddressed.\n",
       "\n",
       "LeCun acknowledges the impressive capabilities of LLMs, particularly in self-supervised learning, which has demonstrated significant advancements in language understanding and translation. However, he maintains that these models still lack the deep understanding of the physical world necessary for true intelligence. He critiques the Turing test as an inadequate measure of intelligence, emphasizing the need to recognize the limitations of current AI systems while acknowledging their impressive capabilities.\n",
       "\n",
       "LeCun concludes that to achieve human-level AI, it is essential to move beyond generative models and focus on joint embedding and predictive approaches that can better capture the complexities of the real world. He argues that high-level reasoning in LLMs is fundamentally different from the common-sense reasoning required to navigate the physical world, and that a robust understanding of low-level experiences is necessary to build a consistent world model. This foundational knowledge, which LLMs currently lack, is crucial for developing AI systems that can effectively understand and interact with the world. He also notes that the autoregressive nature of LLMs leads to a constant computational effort per token produced, which limits their ability to adaptively allocate resources for more complex queries, further underscoring their limitations in reasoning and planning compared to human cognition.\n",
       "\n",
       "LeCun further elaborates on the future of AI systems, suggesting that they will differ significantly from autoregressive LLMs. He draws a parallel between human cognitive processes, distinguishing between \"system one\" tasks, which are instinctive and require little conscious thought, and \"system two\" tasks, which involve deliberate planning and reasoning. He posits that future AI systems will need to incorporate a planning mechanism that allows them to allocate resources based on the complexity of the task at hand, moving away from the autoregressive prediction of tokens. Instead, he envisions a model that optimizes answers in an abstract representation space, using energy-based models to evaluate the quality of responses. This approach would enable AI systems to engage in more sophisticated reasoning and planning, ultimately leading to a more human-like understanding of the world. LeCun recommends abandoning generative models, autoregressive generation, and probabilistic models in favor of joint embedding architectures and energy-based models. He also suggests minimizing the use of reinforcement learning (RL), advocating for model predictive control as a more efficient alternative, using RL only when necessary to adjust world models or objectives.\n",
       "\n",
       "In light of recent criticisms of AI systems, such as Google's Gemini 1.5, LeCun addresses the issue of bias in AI. He argues that it is impossible to create an unbiased AI system, as bias is subjective and varies among individuals. He advocates for open-source AI as a solution, emphasizing the need for diverse perspectives in AI development, akin to the principles of free speech and a free press in a democracy. LeCun envisions a future where AI mediates our interactions with the digital world, underscoring the importance of transparency and diversity in AI systems to foster a more equitable and informed society. He stresses that the concentration of AI power in a few companies poses a danger to democracy and local cultures, advocating for open-source platforms that allow diverse groups to fine-tune AI systems for their specific needs, thereby promoting a rich ecosystem of AI applications that reflect varied languages, cultures, and values. LeCun also discusses Meta's business model, suggesting that providing open-source models can benefit the company by allowing businesses to build applications on top of them, ultimately enhancing Meta's offerings and revenue potential without compromising the distribution of foundational models. He further notes that open-source AI enables diversity, allowing for tailored models that can cater to different political and cultural perspectives, which could lead to a more nuanced and effective interaction with technology.\n",
       "\n",
       "LeCun expresses excitement about the potential for human-level intelligence through advancements in AI, particularly in the context of collaborative efforts with researchers at institutions like DeepMind and Berkeley. He acknowledges the importance of both computational scale and architectural innovation in achieving these goals, while also recognizing the need for hardware improvements to match the efficiency of the human brain. He emphasizes that the journey toward artificial general intelligence (AGI) will be gradual, rather than a sudden breakthrough, and that significant progress is still required in learning representations and understanding the world before reaching human-level capabilities. He cautions that achieving a fully integrated system capable of reasoning, planning, and learning in a hierarchical manner will take at least a decade, if not longer, due to the numerous challenges that remain unaddressed in the field.\n",
       "\n",
       "LeCun also addresses concerns about the potential dangers of advanced AI systems, arguing against the notion that intelligent systems will inherently seek to dominate or harm humans. He explains that AI systems are not species and do not possess the hardwired desires for dominance found in social species. Instead, he believes that AI can be designed to be submissive to human control, with guardrails in place to ensure safe operation. He acknowledges the complexity of designing these guardrails and emphasizes the need for iterative development to refine AI behavior over time. Drawing an analogy to the evolution of turbojet safety, he suggests that just as engineers have progressively improved turbojet reliability, AI systems can be developed to be controllable and safe through careful design and ongoing adjustments. He also discusses the potential for AI systems to mediate interactions in the digital world, acting as filters to protect users from manipulative or harmful content, thereby enhancing the safety and reliability of AI technologies.\n",
       "\n",
       "LeCun reflects on the human psychology surrounding new technologies, likening the skepticism towards AI to historical fears of innovations like electricity or trains. He notes that such fears often stem from a natural instinct to protect cultural norms and societal structures from perceived threats. He emphasizes the importance of embracing change and understanding the real versus imagined dangers of new technologies. LeCun reiterates the necessity of open-source platforms to democratize AI development and prevent the concentration of power in a few large companies, which could exploit the technology to the detriment of society. He advocates for a diverse set of voices in AI development to ensure that the technology reflects a wide range of cultural values and perspectives, ultimately fostering a more equitable and informed society. He warns that the concentration of AI power in proprietary systems poses a significant threat to democracy and the diversity of ideas, advocating for open-source solutions to preserve a rich ecosystem of thought and innovation.\n",
       "\n",
       "LeCun also highlights the complexity of tasks that AI systems need to perform in real-world environments, such as cleaning, cooking, and navigating spaces filled with uncertainty. While current AI systems can perform specific tasks, like navigating to a fridge or picking up objects, they are not yet capable of generalizing these skills to more complex tasks, such as clearing a dinner table. He expresses hope for the future of humanoid robots and their potential to enhance human interaction with AI in physical spaces, allowing for deeper philosophical and psychological exploration of our relationships with robots. He encourages innovative research in areas like self-supervised learning from video and planning with learned world models, emphasizing that significant advancements can be made without necessarily relying on large datasets. LeCun believes that the future of AI will involve hierarchical planning and the ability to plan actions in various contexts, not just physical ones, which remains an area with much room for exploration and development.\n",
       "\n",
       "LeCun envisions a future where AI amplifies human intelligence, likening it to the transformative impact of the printing press, which democratized knowledge and fostered enlightenment. He believes that AI can enhance human capabilities, allowing individuals to manage a \"staff\" of intelligent assistants that can perform tasks more efficiently. This potential for AI to improve human decision-making and knowledge dissemination gives him hope for the future, despite the challenges posed by societal divisions and conflicts. He argues that, like the printing press, AI has the potential to elevate humanity, provided it is developed and integrated thoughtfully into society."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(summary['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d6815-fb68-45d5-b8aa-f950d8906e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7087483-0dc8-4ccd-9dac-cbfae8931555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7474f-daa8-4a95-a827-7f9355e949d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dfb8f-e99e-4b9a-8044-d1d95611c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4e571-c29e-4bab-ba01-be1b4803b9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e4517-b750-4ed9-beee-c8c4147f0c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
