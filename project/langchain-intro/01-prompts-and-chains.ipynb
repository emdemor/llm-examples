{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b32d977-08a9-4129-85eb-277f656ae201",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b82e54-996a-42c3-b191-5de1bd44eb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert your openai api key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Insert your openai api key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240bea1-033f-4da0-bbc3-2624851bcaf6",
   "metadata": {},
   "source": [
    "O LangChain fornece muitos módulos que podem ser usados para construir aplicativos de modelo de linguagem. Os módulos podem ser combinados para criar aplicativos mais complexos ou usados individualmente para aplicativos simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b9695-0041-4acc-9631-13bc5f584794",
   "metadata": {},
   "source": [
    "O bloco de construção mais básico do LangChain é chamar um LLM em algum input. Vamos ver um exemplo simples de como fazer isso. Para isso, vamos fingir que estamos construindo um serviço que gera um nome de empresa com base no que a empresa faz.\n",
    "\n",
    "Para fazer isso, primeiro precisamos importar o wrapper LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edc42f4-dc85-418f-b312-7487b88d5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa56ab-57f0-4534-9dcb-45bc9f74d356",
   "metadata": {},
   "source": [
    "Podemos então inicializar o wrapper com quaisquer argumentos. Neste exemplo, provavelmente queremos que as saídas sejam MAIS dispersas (duas chamadas irão gerar resultados distindos; pode-se pensar em um modelo mais \"criativo\"), então vamos inicializá-las com uma temperatura ALTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78b4137-998b-463b-9161-755779282f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133e47fc-175c-4d9f-beb4-ce995f243b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentativa 0: Algo Color - Meias Coloridas.\n",
      "Tentativa 1: Rainbow Socks Co.\n",
      "Tentativa 2: KaleidoSocks.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    text = \"Qual seria um bom nome para uma empresa que faz meias coloridas?\"\n",
    "    print(f\"Tentativa {i}: {llm(text).strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4bcb0-3df8-4d79-94b1-e5dc5a5092f1",
   "metadata": {},
   "source": [
    "Diminuindo a temperatura, as respostas tendem a ter menos variação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858ddc29-ca85-40fc-b25d-136ef6b76e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentativa 0: Colorful Socks Co.\n",
      "Tentativa 1: Colorful Socks Co.\n",
      "Tentativa 2: Colorful Socks Co.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.0)\n",
    "for i in range(3):\n",
    "    text = \"Qual seria um bom nome para uma empresa que faz meias coloridas?\"\n",
    "    print(f\"Tentativa {i}: {llm(text).strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658d995-2c2c-497c-8def-2a37e6da76f4",
   "metadata": {},
   "source": [
    "# PromptTemplates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9357426c-56c7-48e0-aa59-cc8a115db2d5",
   "metadata": {},
   "source": [
    "Chamar um LLM é um ótimo primeiro passo, mas é apenas o começo. Normalmente, quando você usa um LLM em um aplicativo, não está enviando a entrada do usuário diretamente para o LLM. Em vez disso, você provavelmente está recebendo a entrada do usuário e construindo um prompt e, em seguida, enviando-o para o LLM.\n",
    "\n",
    "Por exemplo, no exemplo anterior, o texto que passamos foi codificado para solicitar o nome de uma empresa que fabricava meias coloridas. Nesse serviço imaginário, o que gostaríamos de fazer é pegar apenas a entrada do usuário descrevendo o que a empresa faz e, em seguida, formatar o prompt com essas informações.\n",
    "\n",
    "Isso é fácil de fazer com LangChain!\n",
    "\n",
    "Primeiro, vamos definir o modelo de prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9862eb0a-c3c0-44e6-9a08-4df2535006f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "text = \"Qual seria um bom nome para uma empresa que faz {product}?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fba49-4a0e-453f-95f6-ace38102cf58",
   "metadata": {},
   "source": [
    "Podemos chamar o método format para obter o prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cad1793-215a-4848-b708-d783492a29e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qual seria um bom nome para uma empresa que faz meias coloridas?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(product=\"meias coloridas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d2f2e-de2c-4fe0-a878-077f779777a8",
   "metadata": {},
   "source": [
    "# Chains: combine LLMs e prompts em fluxos de trabalho de várias etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd4c7fb-1b8f-4634-a450-cb0902c5e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model='text-davinci-003', temperature=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1118ef57-ca2a-4620-babe-ceb4d82cbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c6a27b-ed0a-48d8-bde4-2b66c17a8aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Color Sox.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"meias coloridas\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7726c6-ab49-42dd-8807-ead46bfd5653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tampas Criativas'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"tampas de caneta\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138f617f-8404-4679-beb4-a53653f6e097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uma boa ideia seria o \"Óleo Nectar\".'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"garrafas de óleo\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae37e804-471a-4b4e-9339-a2659b0865f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GloveCare.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"luvas\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6175d-d47e-489f-83c5-2ddeea035094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4c043-5952-44a5-b542-f972877e3755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a9dc2-b41e-41be-9386-6fa852a23f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24c32d-4ead-4392-b8fd-79d8b3e40922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad90c54-f5b2-49af-b72e-989b526a7fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3377205-1fea-4120-a957-593460dfe38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cac15-5708-4bd6-964e-1a2762365e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db872f3e-99ae-47c2-a828-58d48b95a876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6a7ec-9068-436a-99e5-672874412759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fb5c3-6a97-4603-aba6-7aba1d30019a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626105f-23c1-42ae-8fa8-023fe4ecad00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eafeb0-84ad-4f07-8f21-cc593997c237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd6a4c-5a07-49b2-a57b-4d079432830e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa0a32-f55d-4df8-9e93-a39f95f53373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
